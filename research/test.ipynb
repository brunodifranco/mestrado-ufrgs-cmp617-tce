{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"src\")\n",
    "from data_cleaning import DataCleaning\n",
    "from datasets import Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_path = \"/home/brunodifranco/mestrado/mestrado-ufrgs-cmp617-tce/src/utils/stop_words_bertopic.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 01:01:04 - INFO - Data loaded!\n",
      "2024-04-20 01:01:04 - INFO - Null values cleaned!\n",
      "2024-04-20 01:01:04 - INFO - Data types asserted!\n",
      "2024-04-20 01:01:04 - INFO - Full data cleaned!\n"
     ]
    }
   ],
   "source": [
    "data_cleaning_pipeline = DataCleaning()\n",
    "df = data_cleaning_pipeline.run()\n",
    "\n",
    "data = Dataset.from_pandas(df)\n",
    "docs = data[\"DS_OBJETO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open(stop_words_path, \"r\") as file:\n",
    "    for row in file:\n",
    "        stop_words.append(row.strip())\n",
    "\n",
    "\n",
    "# vectorizer_model = CountVectorizer(\n",
    "#     stop_words=stop_words,\n",
    "#     strip_accents=\"unicode\",\n",
    "#     token_pattern=r\"(?u)\\\\b[A-Za-z]+\\\\b\",  # Remove numbers\n",
    "# )\n",
    "# # Filtering docs that are completely null after stopwords removal\n",
    "# vec = vectorizer_model.fit_transform(docs)\n",
    "# doc_lengths = vec.sum(axis=1)\n",
    "# non_empty_docs = doc_lengths > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunodifranco/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'sera', 'seriamos', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Lista de stop words, ajuste conforme necessário\n",
    "# stop_words = ['a', 'e', 'o', 'as', 'os']\n",
    "\n",
    "# vectorizer_model = CountVectorizer(\n",
    "#     stop_words=stop_words,\n",
    "#     strip_accents=\"unicode\",\n",
    "#     token_pattern=r'(?u)\\b[A-Za-zÀ-ÿ]+\\b'  # Regex ajustado para aceitar letras com e sem acentos, excluindo números\n",
    "    \n",
    "# )\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    strip_accents=\"unicode\",\n",
    "    token_pattern=r'(?u)\\b[A-Za-zÀ-ÿ]{4,}\\b'  # Regex ajustado para aceitar letras com 4 ou mais caracteres\n",
    ")\n",
    "\n",
    "\n",
    "# # Supondo que corpus seja uma lista de documentos de texto\n",
    "# corpus = [\n",
    "#     \"com número 1234\",\n",
    "#     \"Outro  sem número\",\n",
    "#     \"Apenas um asgfdgdrfgdf6789\"\n",
    "# ]\n",
    "\n",
    "X = vectorizer_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['apenas', 'com', 'exemplo', 'numero', 'outro', 'sem', 'um'],\n",
       "       dtype=object),\n",
       " array([[0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Lista de stop words, ajuste conforme necessário\n",
    "stop_words = ['a', 'e', 'o', 'as', 'os']\n",
    "\n",
    "# Definindo o modelo com as configurações atualizadas\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=stop_words,\n",
    "    strip_accents=\"unicode\",\n",
    "    token_pattern=r'(?u)\\b[A-Za-zÀ-ÿ]+\\b'  # Regex ajustado para aceitar letras com e sem acentos, excluindo números\n",
    ")\n",
    "\n",
    "# Supondo que corpus seja uma lista de documentos de texto\n",
    "corpus = [\n",
    "    \"Exemplo com número 1234\",\n",
    "    \"Outro exemplo sem número\",\n",
    "    \"Apenas um exemplo 6789\"\n",
    "]\n",
    "\n",
    "# Aplicando o vectorizer ao corpus\n",
    "X = vectorizer_model.fit_transform(corpus)\n",
    "feature_names = vectorizer_model.get_feature_names_out()\n",
    "matrix = X.toarray()\n",
    "\n",
    "feature_names, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunodifranco/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['eramos', 'estao', 'estavamos', 'estiveramos', 'estivessemos', 'foramos', 'fossemos', 'ha', 'hao', 'houveramos', 'houverao', 'houveriamos', 'houvessemos', 'ja', 'sera', 'seriamos', 'so', 'tambem', 'tera', 'terao', 'teriamos', 'tinhamos', 'tiveramos', 'tivessemos', 'voce', 'voces'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vec = vectorizer_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = vec.sum(axis=1)\n",
    "non_empty_docs = doc_lengths > 0\n",
    "\n",
    "# Filtre o DataFrame original e a matriz de documentos\n",
    "df_filtered = df[non_empty_docs]\n",
    "# vec_filtered = vec[non_empty_docs]\n",
    "\n",
    "# # Atualize o DataFrame `df` e a matriz `vec` com os documentos não nulos\n",
    "# df = df_filtered\n",
    "# vec = vec_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_pandas(df_filtered)\n",
    "docs = data[\"DS_OBJETO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"models/bertopic/inputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(docs, open(path + \"docs.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
