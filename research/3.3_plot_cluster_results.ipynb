{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "params_path = \"/home/bruno/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/src/opt_outputs/results_stemmer_with_filter_500.json\"\n",
    "\n",
    "with open(params_path, \"r\") as json_file:\n",
    "    params_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union, List, Dict\n",
    "from logging import Logger\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from data_cleaning import DataCleaning\n",
    "from utils.utils import logger\n",
    "from utils.nlp import preprocess, remove_stop_words, stemmer_pt, lemma_pt\n",
    "import spacy\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 20:19:35 - INFO - Data loaded!\n",
      "2024-04-03 20:19:35 - INFO - Null values cleaned!\n",
      "2024-04-03 20:19:35 - INFO - Data types asserted!\n",
      "2024-04-03 20:19:35 - INFO - Full data cleaned!\n"
     ]
    }
   ],
   "source": [
    "cleaning_pipeline = DataCleaning()\n",
    "df = cleaning_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    DS_OBJETO_NLP=df[\"DS_OBJETO\"]\n",
    "    .apply(\n",
    "        lambda x: nltk.word_tokenize(x.lower(), language=\"portuguese\")\n",
    "    )  # Tokenize\n",
    "    .apply(lambda x: [preprocess(word) for word in x])  # Other preprocessing\n",
    "    .apply(lambda x: list(filter(None, x)))  # Removes items with none\n",
    "    .apply(remove_stop_words)  # Removes stop words\n",
    "    .apply(\n",
    "        lambda x: [word for word in x if \"rs\" not in word]\n",
    "    )  # Remove tokens containing \"rs\" (which are cities)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128753/128753 [00:46<00:00, 2748.36it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df[\"DS_OBJETO_NLP\"] = df[\"DS_OBJETO_NLP\"].progress_apply(\n",
    "    stemmer_pt\n",
    ")  # Applies stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = df[\"DS_OBJETO_NLP\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_normalization_method = params_dict[\"nlp_normalization_method\"]\n",
    "n_filter = params_dict[\"filter\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(vec)\n",
    "tokens_a_remover = [\n",
    "    token for token, freq in id2word.dfs.items() if freq < n_filter\n",
    "]\n",
    "id2word.filter_tokens(bad_ids=tokens_a_remover)\n",
    "id2word.compactify()\n",
    "corpus = [id2word.doc2bow(text) for text in vec]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = params_dict[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params[\"corpus\"] = corpus\n",
    "model_params[\"id2word\"] = id2word\n",
    "model_params[\"per_word_topics\"] = True\n",
    "model_params[\"random_state\"] = 42 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaMulticore(**model_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0, 60):\n",
    "\n",
    "#     print(f\"Running iteration number {i}\")\n",
    "\n",
    "#     lda_model = LdaMulticore(**model_params)\n",
    "#     coherence_model_lda = CoherenceModel(\n",
    "#         model=lda_model, texts=vec, dictionary=id2word, coherence=\"c_v\"\n",
    "#     )\n",
    "#     coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "#     pickle.dump(lda_model, open(f'models/model_{str(coherence_lda).lstrip(\"0.\")}.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = pickle.load(open('models/lda/model_5348854098362079.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=vec, dictionary=id2word, coherence=\"c_v\"\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lda_model, open('models/model_novo_05347.pkl', 'wb')) # Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5347081193510318'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(coherence_lda).lstrip(\"0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGUIR CONSTRUINDO O CODIGO DO SCRIPPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.176*\"veicul\" + 0.077*\"maquin\" + 0.057*\"frot\" + 0.037*\"ole\" + 0.035*\"combusti\" + 0.029*\"plac\" + 0.029*\"corre\" + 0.026*\"empr\" + 0.025*\"diesel\" + 0.024*\"abastec\"'),\n",
       " (1,\n",
       "  '0.064*\"eletr\" + 0.039*\"cont\" + 0.036*\"port\" + 0.030*\"equip\" + 0.030*\"iluminaca\" + 0.027*\"proteca\" + 0.026*\"min\" + 0.026*\"caminha\" + 0.025*\"abert\" + 0.024*\"agricol\"'),\n",
       " (2,\n",
       "  '0.100*\"serv\" + 0.048*\"transport\" + 0.043*\"tecn\" + 0.040*\"agu\" + 0.037*\"instalaca\" + 0.023*\"informa\" + 0.022*\"integr\" + 0.018*\"estabelec\" + 0.017*\"sol\" + 0.017*\"descrit\"'),\n",
       " (3,\n",
       "  '0.152*\"saud\" + 0.098*\"medic\" + 0.059*\"famil\" + 0.051*\"eletron\" + 0.038*\"basic\" + 0.025*\"hospital\" + 0.025*\"hospit\" + 0.024*\"odontolog\" + 0.020*\"paci\" + 0.018*\"modal\"'),\n",
       " (4,\n",
       "  '0.169*\"escol\" + 0.075*\"alimentici\" + 0.072*\"educaca\" + 0.056*\"agricult\" + 0.048*\"ensin\" + 0.045*\"famili\" + 0.041*\"alimentaca\" + 0.035*\"cult\" + 0.033*\"rural\" + 0.032*\"infantil\"'),\n",
       " (5,\n",
       "  '0.075*\"limp\" + 0.065*\"obr\" + 0.047*\"pneu\" + 0.045*\"hidraul\" + 0.041*\"predi\" + 0.033*\"movel\" + 0.033*\"seguranc\" + 0.031*\"higien\" + 0.026*\"pint\" + 0.025*\"produt\"'),\n",
       " (6,\n",
       "  '0.072*\"rua\" + 0.057*\"construca\" + 0.048*\"pavimentaca\" + 0.041*\"empreit\" + 0.038*\"reform\" + 0.037*\"concret\" + 0.035*\"local\" + 0.027*\"asfal\" + 0.026*\"urban\" + 0.023*\"pedr\"')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModellingLDA:\n",
    "    \"\"\"\n",
    "    Performs Latent Dirichlet Allocation (LDA) model on selected parameters.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    nlp_normalization_method : str\n",
    "        NLP normalization method to choose: either 'stemmer' or 'lemmatization'\n",
    "    n_filter : int\n",
    "        Minimum frequency to retain a token in the dictionary.\n",
    "    n_trials : int\n",
    "        Number of trials in optimization.\n",
    "    logger : Logger, defaults to logger\n",
    "        logger.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp_normalization_method: str,\n",
    "        n_filter: int,\n",
    "        n_trials: int,\n",
    "        logger: Logger = logger,\n",
    "    ):\n",
    "\n",
    "        self.nlp_normalization_method = nlp_normalization_method\n",
    "        self.n_filter = n_filter\n",
    "        self.n_trials = n_trials\n",
    "        self.logger = logger\n",
    "\n",
    "    def nlp_preprocessing(self) -> List[str]:\n",
    "        cleaning_pipeline = DataCleaning()\n",
    "        df = cleaning_pipeline.run()\n",
    "\n",
    "        self.logger.info(\"Running NLP treatment\")\n",
    "\n",
    "        df = df.assign(\n",
    "            DS_OBJETO_NLP=df[\"DS_OBJETO\"]\n",
    "            .apply(\n",
    "                lambda x: nltk.word_tokenize(x.lower(), language=\"portuguese\")\n",
    "            )  # Tokenize\n",
    "            .apply(lambda x: [preprocess(word) for word in x])  # Other preprocessing\n",
    "            .apply(lambda x: list(filter(None, x)))  # Removes items with none\n",
    "            .apply(remove_stop_words)  # Removes stop words\n",
    "            .apply(\n",
    "                lambda x: [word for word in x if \"rs\" not in word]\n",
    "            )  # Remove tokens containing \"rs\" (which are cities)\n",
    "        )\n",
    "\n",
    "        if self.nlp_normalization_method == \"stemmer\":\n",
    "            self.logger.info(\"Running stemmer\")\n",
    "\n",
    "            tqdm.pandas()\n",
    "            df[\"DS_OBJETO_NLP\"] = df[\"DS_OBJETO_NLP\"].progress_apply(\n",
    "                stemmer_pt\n",
    "            )  # Applies stemming\n",
    "\n",
    "        elif self.nlp_normalization_method == \"lemmatization\":\n",
    "            self.logger.info(\"Running lemmatization\")\n",
    "\n",
    "            nlp = spacy.load(\"pt_core_news_md\", disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "            tqdm.pandas()\n",
    "            df[\"DS_OBJETO_NLP\"] = df[\"DS_OBJETO_NLP\"].progress_apply(\n",
    "                lambda x: lemma_pt(nlp, x)\n",
    "            )  # Applies lemmatization\n",
    "\n",
    "        else:\n",
    "            self.logger.error(\"TypeError\")\n",
    "            raise TypeError(\n",
    "                \"Please choose either 'stemmer' or 'lemmatization' as the nlp_normalization_method\"\n",
    "            )\n",
    "\n",
    "        vec = df[\"DS_OBJETO_NLP\"].values.tolist()\n",
    "        return vec\n",
    "    \n",
    "    def create_corpus(\n",
    "        self, vec: List[str]\n",
    "    ) -> Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]:\n",
    "        \"\"\"\n",
    "        Perform optimization of LDA (Latent Dirichlet Allocation) model parameters using Bayesian Optimization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vec : List[str]\n",
    "            List of text data for model optimization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]\n",
    "        \"\"\"\n",
    "\n",
    "        # Create corpus\n",
    "        id2word = Dictionary(vec)\n",
    "        tokens_a_remover = [\n",
    "            token for token, freq in id2word.dfs.items() if freq < self.n_filter\n",
    "        ]\n",
    "        id2word.filter_tokens(bad_ids=tokens_a_remover)\n",
    "        id2word.compactify()\n",
    "        corpus = [id2word.doc2bow(text) for text in vec]\n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    def fit(\n",
    "        self, vec: List[str]\n",
    "    ) -> Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]:\n",
    "        \"\"\"\n",
    "        Perform optimization of LDA (Latent Dirichlet Allocation) model parameters using Bayesian Optimization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vec : List[str]\n",
    "            List of text data for model optimization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]\n",
    "        \"\"\"\n",
    "\n",
    "        # Create corpus\n",
    "        id2word = Dictionary(vec)\n",
    "        tokens_a_remover = [\n",
    "            token for token, freq in id2word.dfs.items() if freq < self.n_filter\n",
    "        ]\n",
    "        id2word.filter_tokens(bad_ids=tokens_a_remover)\n",
    "        id2word.compactify()\n",
    "        corpus = [id2word.doc2bow(text) for text in vec]\n",
    "\n",
    "        return corpus\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
