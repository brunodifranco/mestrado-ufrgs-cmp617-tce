{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from logging import Logger\n",
    "from utils.utils import logger\n",
    "from gensim.models import CoherenceModel, ldamulticore\n",
    "from gensim.corpora import Dictionary\n",
    "import json\n",
    "from typing import Union, List, Dict\n",
    "import optuna\n",
    "# from data_cleaning import DataCleaning\n",
    "from utils.nlp import preprocess, remove_stop_words, stemmer_pt, lemma_pt\n",
    "\n",
    "\n",
    "class LDAOptimization:\n",
    "    \"\"\"\n",
    "    Performs Bayesian Optimization on Latent Dirichlet Allocation (LDA) model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    nlp_normalization_method : str\n",
    "        NLP normalization method to choose: either 'stemmer' or 'lemmatization'\n",
    "    n_filter : int\n",
    "        Minimum frequency to retain a token in the dictionary.\n",
    "    n_trials : int\n",
    "        Number of trials in optimization.\n",
    "    logger : Logger, defaults to logger\n",
    "        logger.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp_normalization_method: str,\n",
    "        n_filter: int,\n",
    "        n_trials: int,\n",
    "        logger: Logger = logger,\n",
    "    ):\n",
    "\n",
    "        self.nlp_normalization_method = nlp_normalization_method\n",
    "        self.n_filter = n_filter\n",
    "        self.n_trials = n_trials\n",
    "        self.logger = logger\n",
    "\n",
    "    def nlp_preprocessing(self) -> List[str]:\n",
    "        cleaning_pipeline = DataCleaning()\n",
    "        df = cleaning_pipeline.run()\n",
    "\n",
    "        # TODO - REMOVER AQUI DEPOIS\n",
    "        df = df[df[\"ANO_LICITACAO\"] >= 2021]\n",
    "\n",
    "        if self.nlp_normalization_method == \"stemmer\":\n",
    "\n",
    "            df = df.assign(\n",
    "                DS_OBJETO_NLP=df[\"DS_OBJETO\"]\n",
    "                .apply(\n",
    "                    lambda x: nltk.word_tokenize(x.lower(), language=\"portuguese\")\n",
    "                )  # Tokenize\n",
    "                .apply(\n",
    "                    lambda x: [preprocess(word) for word in x]\n",
    "                )  # Other preprocessing\n",
    "                .apply(lambda x: list(filter(None, x)))  # Removes items with none\n",
    "                .apply(remove_stop_words)  # Removes stop words\n",
    "                .apply(\n",
    "                    lambda x: [word for word in x if \"rs\" not in word]\n",
    "                )  # Remove tokens containing \"rs\" (which are cities)\n",
    "                .apply(stemmer_pt)  # Applies stemming\n",
    "            )\n",
    "\n",
    "            vec = df[\"DS_OBJETO_NLP\"].values.tolist()\n",
    "\n",
    "            self.logger.info(\"Vector with stemmer created\")\n",
    "            return vec\n",
    "\n",
    "        if self.nlp_normalization_method == \"lemmatization\":\n",
    "\n",
    "            df = df.assign(\n",
    "                DS_OBJETO_NLP=df[\"DS_OBJETO\"]\n",
    "                .apply(\n",
    "                    lambda x: nltk.word_tokenize(x.lower(), language=\"portuguese\")\n",
    "                )  # Tokenize\n",
    "                .apply(\n",
    "                    lambda x: [preprocess(word) for word in x]\n",
    "                )  # Other preprocessing\n",
    "                .apply(lambda x: list(filter(None, x)))  # Removes items with none\n",
    "                .apply(remove_stop_words)  # Removes stop words\n",
    "                .apply(\n",
    "                    lambda x: [word for word in x if \"rs\" not in word]\n",
    "                )  # Remove tokens containing \"rs\" (which are cities)\n",
    "                .apply(lemma_pt)  # Applies lemmatization\n",
    "            )\n",
    "\n",
    "            vec = df[\"DS_OBJETO_NLP\"].values.tolist()\n",
    "\n",
    "            self.logger.info(\"Vector with lemmatization created\")\n",
    "            return vec\n",
    "\n",
    "        else:\n",
    "            self.logger.error(\"TypeError\")\n",
    "            raise TypeError(\n",
    "                \"Please choose either 'stemmer' or 'lemmatization' as the nlp_normalization_method\"\n",
    "            )\n",
    "\n",
    "    def bayesian_opt_objective(\n",
    "        self, trial: optuna.Trial, id2word: Dictionary, corpus: List, vec: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Objective function for Bayesian Optimization to tune parameters for LDA (Latent Dirichlet Allocation) model,\n",
    "        returning the Coherence score c-v (Cosine Similarity).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trial : optuna.Trial\n",
    "            A single trial of an optimization experiment. The objective function uses this to suggest new parameters.\n",
    "        id2word : gensim.corpora.Dictionary\n",
    "            Gensim dictionary mapping of word IDs to words.\n",
    "        corpus : List\n",
    "            List of Bag-of-Words corpus.\n",
    "        vec : List[str]\n",
    "            List of text data for model coherence calculation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Coherence score c-v (Cosine Similarity) of the LDA model.\n",
    "        \"\"\"\n",
    "\n",
    "        num_topics = trial.suggest_int(\"num_topics\", 5, 7, step=1)\n",
    "        chunksize = trial.suggest_int(\"chunksize\", 80, 180, step=10)\n",
    "        passes = trial.suggest_int(\"passes\", 5, 20, step=1)\n",
    "        alpha = trial.suggest_float(\"alpha\", 0.01, 1, step=0.01)\n",
    "        eta = trial.suggest_float(\"eta\", 0.01, 0.91, step=0.01)\n",
    "        decay = trial.suggest_float(\"decay\", 0.5, 1, step=0.1)\n",
    "\n",
    "        lda_model = ldamulticore.LdaMulticore(\n",
    "            corpus=corpus,\n",
    "            id2word=id2word,\n",
    "            num_topics=num_topics,\n",
    "            chunksize=chunksize,\n",
    "            passes=passes,\n",
    "            alpha=alpha,\n",
    "            eta=eta,\n",
    "            decay=decay,\n",
    "            random_state=42,\n",
    "            per_word_topics=True,\n",
    "        )\n",
    "\n",
    "        # Coherence Score\n",
    "        coherence_model_lda = CoherenceModel(\n",
    "            model=lda_model, texts=vec, dictionary=id2word, coherence=\"c_v\"\n",
    "        )\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        return coherence_lda\n",
    "\n",
    "    def get_opt(\n",
    "        self, vec: List[str]\n",
    "    ) -> Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]:\n",
    "        \"\"\"\n",
    "        Perform optimization of LDA (Latent Dirichlet Allocation) model parameters using Bayesian Optimization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vec : List[str]\n",
    "            List of text data for model optimization.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]\n",
    "        \"\"\"\n",
    "\n",
    "        # Create corpus\n",
    "        id2word = Dictionary(vec)\n",
    "        tokens_a_remover = [\n",
    "            token for token, freq in id2word.dfs.items() if freq < self.n_filter\n",
    "        ]\n",
    "        id2word.filter_tokens(bad_ids=tokens_a_remover)\n",
    "        id2word.compactify()\n",
    "        corpus = [id2word.doc2bow(text) for text in vec]\n",
    "\n",
    "        # Optimizer\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(\n",
    "            lambda trial: self.bayesian_opt_objective(trial, id2word, corpus, vec),\n",
    "            n_trials=self.n_trials,\n",
    "        )\n",
    "        trial = study.best_trial\n",
    "\n",
    "        # Store results\n",
    "        results = {}\n",
    "        results[\"filter\"] = self.n_filter\n",
    "        results[\"nlp_normalization_method\"] = self.nlp_normalization_method\n",
    "        results[\"best_score\"] = trial.value\n",
    "        results[\"params\"] = trial.params\n",
    "\n",
    "        self.logger.info(\"Results stored\")\n",
    "        return results\n",
    "\n",
    "    def save_results(\n",
    "        self, results: Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save results to JSON.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        results: Dict[str, Union[int, str, float, Dict[str, Union[int, float]]]]\n",
    "            Dict with results.\n",
    "        \"\"\"\n",
    "\n",
    "        output_path = f\"src/opt_outputs/results_{self.nlp_normalization_method}_with_filter_{self.n_filter}.json\"\n",
    "        with open(output_path, \"w\") as json_file:\n",
    "            json.dump(results, json_file)\n",
    "\n",
    "        self.logger.info(\"Results saved to JSON\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the optimizer\n",
    "        \"\"\"\n",
    "        vec = self.nlp_preprocessing()\n",
    "        results = self.get_opt(vec)\n",
    "        self.save_results(results)\n",
    "\n",
    "        self.logger.info(\"Optimizer completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bruno/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/src'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = LDAOptimization(\n",
    "    nlp_normalization_method=\"lemmatization\", n_filter=250, n_trials=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 18:34:02 - INFO - Data loaded!\n",
      "2024-04-02 18:34:02 - INFO - Null values cleaned!\n",
      "2024-04-02 18:34:02 - INFO - Data types asserted!\n",
      "2024-04-02 18:34:02 - INFO - Full data cleaned!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/utils/stop_words.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vec \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# results = self.get_opt(vec)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# self.save_results(results)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 83\u001b[0m, in \u001b[0;36mLDAOptimization.nlp_preprocessing\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vec\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp_normalization_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatization\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39massign(\n\u001b[1;32m     75\u001b[0m         DS_OBJETO_NLP\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDS_OBJETO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mportuguese\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tokenize\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Other preprocessing\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Removes items with none\u001b[39;49;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stop_words\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Removes stop words\u001b[39;00m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m x: [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m word]\n\u001b[1;32m     86\u001b[0m         )  \u001b[38;5;66;03m# Remove tokens containing \"rs\" (which are cities)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(lemma_pt)  \u001b[38;5;66;03m# Applies lemmatization\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m     vec \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDS_OBJETO_NLP\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector with lemmatization created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/.venv/lib/python3.10/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/.venv/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/.venv/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/.venv/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/src/utils/nlp.py:31\u001b[0m, in \u001b[0;36mremove_stop_words\u001b[0;34m(text, stop_words_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mRemoves stop words from a list of words.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    List of words with stop words removed.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstop_words_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m     33\u001b[0m         stop_words\u001b[38;5;241m.\u001b[39mappend(row\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/utils/stop_words.txt'"
     ]
    }
   ],
   "source": [
    "vec = optimizer.nlp_preprocessing()\n",
    "# results = self.get_opt(vec)\n",
    "# self.save_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.schemas import raw_dtypes\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from pathlib import Path\n",
    "from logging import Logger\n",
    "from utils.utils import logger\n",
    "\n",
    "\n",
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    Cleans TCE-RS data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data_dir: Path, defaults to data/tce_licitations.csv\n",
    "        CSV Data directory.\n",
    "    logger: Logger, defaults to logger\n",
    "        Logger.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, data_dir: Path = \"/home/bruno/mestrado-ufrgs/mestrado-ufrgs-cmp617-tce/data/tce_licitations.csv\", logger: Logger = logger\n",
    "    ):\n",
    "        self.data_dir = data_dir\n",
    "        self.logger = logger\n",
    "\n",
    "    def load(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Loads data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: DataFrame\n",
    "            TCE data.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.data_dir, dtype=raw_dtypes)\n",
    "        self.logger.info(\"Data loaded!\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def clean_nan(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Selects only necessary columns and replaces null values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: DataFrame\n",
    "            TCE data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_cleaned_nan: DataFrame\n",
    "            TCE data with cleaned null values.\n",
    "        \"\"\"\n",
    "\n",
    "        cols_to_keep = [\n",
    "            \"CD_ORGAO\",\n",
    "            \"NM_ORGAO\",\n",
    "            \"ANO_LICITACAO\",\n",
    "            \"DS_OBJETO\",\n",
    "            \"VL_LICITACAO\",\n",
    "            \"DT_HOMOLOGACAO\",\n",
    "            \"VL_HOMOLOGADO\",\n",
    "        ]\n",
    "        df_cleaned_nan = df[cols_to_keep].copy()\n",
    "\n",
    "        df_cleaned_nan.loc[:, \"VL_HOMOLOGADO\"] = df_cleaned_nan[\"VL_HOMOLOGADO\"].fillna(\n",
    "            df_cleaned_nan[\"VL_LICITACAO\"]\n",
    "        )\n",
    "\n",
    "        df_cleaned_nan = df_cleaned_nan.dropna(subset=[\"ANO_LICITACAO\"])\n",
    "\n",
    "        self.logger.info(\"Null values cleaned!\")\n",
    "        return df_cleaned_nan\n",
    "\n",
    "    def asserts_data_types(self, df_cleaned_nan) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Asserting the correct data types.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_cleaned_nan: DataFrame\n",
    "            TCE data with cleaned null values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        df_final: DataFrame\n",
    "            Cleaned DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        # Replaces a few values        \n",
    "        df_final = df_cleaned_nan[~df_cleaned_nan[\"ANO_LICITACAO\"].isin([\"PRD\", \"PDE\"])]\n",
    "        df_final.loc[:, \"ANO_LICITACAO\"] = df_final[\"ANO_LICITACAO\"].replace(\n",
    "            {\"2023.0\": \"2023\", \"2024.0\": \"2024\"}\n",
    "        )\n",
    "        df_final = df_final[\n",
    "            ~df_final[\"VL_HOMOLOGADO\"].isin([\"###############\", \"#################\"])\n",
    "        ]\n",
    "\n",
    "        # Assert data types\n",
    "        df_final[\"CD_ORGAO\"] = df_final[\"CD_ORGAO\"].astype(int)\n",
    "        df_final[\"ANO_LICITACAO\"] = df_final[\"ANO_LICITACAO\"].astype(int)\n",
    "        df_final[\"VL_LICITACAO\"] = df_final[\"VL_LICITACAO\"].astype(float)\n",
    "        df_final[\"DT_HOMOLOGACAO\"] = pd.to_datetime(df_final[\"DT_HOMOLOGACAO\"])\n",
    "        df_final[\"VL_HOMOLOGADO\"] = df_final[\"VL_HOMOLOGADO\"].astype(float)\n",
    "\n",
    "        self.logger.info(\"Data types asserted!\")\n",
    "        return df_final\n",
    "\n",
    "    def run(self) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Executes the full process\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        df_final: DataFrame\n",
    "            Cleaned DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.load()\n",
    "        df_cleaned_nan = self.clean_nan(df)\n",
    "        df_final = self.asserts_data_types(df_cleaned_nan)\n",
    "\n",
    "        self.logger.info(\"Full data cleaned!\")\n",
    "\n",
    "        return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 18:35:43 - INFO - Data loaded!\n",
      "2024-04-02 18:35:43 - INFO - Null values cleaned!\n",
      "2024-04-02 18:35:43 - INFO - Data types asserted!\n",
      "2024-04-02 18:35:43 - INFO - Full data cleaned!\n"
     ]
    }
   ],
   "source": [
    "cleaning_pipeline = DataCleaning()\n",
    "df = cleaning_pipeline.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"ANO_LICITACAO\"] >= 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocessing(df) -> List[str]:\n",
    "    \n",
    "    nlp_normalization_method = \"lemmatization\"\n",
    "\n",
    "    # TODO - REMOVER AQUI DEPOIS\n",
    "    df = df[df[\"ANO_LICITACAO\"] >= 2021]\n",
    "\n",
    "    if nlp_normalization_method == \"stemmer\":\n",
    "        print(\"stemmer\")\n",
    "\n",
    "        df = df.assign(\n",
    "            DS_OBJETO_NLP=df[\"DS_OBJETO\"]\n",
    "            .apply(\n",
    "                lambda x: nltk.word_tokenize(x.lower(), language=\"portuguese\")\n",
    "            )  # Tokenize\n",
    "            .apply(\n",
    "                lambda x: [preprocess(word) for word in x]\n",
    "            )  # Other preprocessing\n",
    "            .apply(lambda x: list(filter(None, x)))  # Removes items with none\n",
    "            .apply(remove_stop_words)  # Removes stop words\n",
    "            .apply(\n",
    "                lambda x: [word for word in x if \"rs\" not in word]\n",
    "            )  # Remove tokens containing \"rs\" (which are cities)\n",
    "            .apply(stemmer_pt)  # Applies stemming\n",
    "        )\n",
    "\n",
    "    elif nlp_normalization_method == \"lemmatization\":\n",
    "        print(\"lemmatization\")\n",
    "\n",
    "        df = df.assign(\n",
    "            DS_OBJETO_NLP=df[\"DS_OBJETO\"]\n",
    "            .apply(\n",
    "                lambda x: nltk.word_tokenize(x.lower(), language=\"portuguese\")\n",
    "            )  # Tokenize\n",
    "            .apply(\n",
    "                lambda x: [preprocess(word) for word in x]\n",
    "            )  # Other preprocessing\n",
    "            .apply(lambda x: list(filter(None, x)))  # Removes items with none\n",
    "            .apply(\n",
    "                lambda x: [word for word in x if \"rs\" not in word]\n",
    "            )  # Remove tokens containing \"rs\" (which are cities)\n",
    "            .apply(lemma_pt)  # Applies lemmatization\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"Please choose either 'stemmer' or 'lemmatization' as the nlp_normalization_method\"\n",
    "        )\n",
    "\n",
    "    vec = df[\"DS_OBJETO_NLP\"].values.tolist()\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vec =  nlp_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    optimizer = LDAOptimization(\n",
    "        nlp_normalization_method=\"lemmatization\", n_filter=250, n_trials=50\n",
    "    )\n",
    "    optimizer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
